{
  "noInstanceSelected": "Aucune instance de mod√®le s√©lectionn√©e",
  "resetToDefault": "R√©initialiser",
  "showAdvancedSettings": "Afficher les param√®tres avanc√©s",
  "showAll": "Tout",
  "basicSettings": "Param√®tres de base",
  "configSubtitle": "Charger ou enregistrer des pr√©r√©glages et exp√©rimenter avec les modifications des param√®tres du mod√®le.",
  "inferenceParameters/title": "Param√®tres de pr√©diction",
  "inferenceParameters/info": "Exp√©rimentez avec les param√®tres qui affectent la pr√©diction.",
  "generalParameters/title": "G√©n√©ral",
  "samplingParameters/title": "√âchantillonnage",
  "basicTab": "Basique",
  "advancedTab": "Avanc√©",
  "advancedTab/title": "üß™ Configuration Avanc√©e",
  "advancedTab/expandAll": "D√©velopper tout",
  "advancedTab/overridesTitle": "Surcharges de configuration",
  "advancedTab/noConfigsText": "Vous n'avez pas de modifications non enregistr√©es - modifiez les valeurs ci-dessus pour voir les surcharges ici.",
  "loadInstanceFirst": "Chargez un mod√®le pour afficher les param√®tres configurables",
  "noListedConfigs": "Aucun param√®tre configurable",
  "generationParameters/info": "Exp√©rimentez avec les param√®tres de base qui affectent la g√©n√©ration de texte.",
  "loadParameters/title": "Charger les param√®tres",
  "loadParameters/description": "Param√®tres pour contr√¥ler l'initialisation et le chargement du mod√®le en m√©moire.",
  "loadParameters/reload": "Recharger pour appliquer les modifications",
  "loadParameters/reload/error": "√âchec du rechargement du mod√®le",
  "discardChanges": "Annuler les modifications",
  "loadModelToSeeOptions": "Chargez un mod√®le pour voir les options",
  "schematicsError.title": "La configuration de sch√©mas contient des erreurs dans les champs suivants :",
  "manifestSections": {
    "structuredOutput/title": "Sortie structur√©e",
    "speculativeDecoding/title": "D√©codage sp√©culatif",
    "sampling/title": "√âchantillonnage",
    "settings/title": "Param√®tres",
    "toolUse/title": "Utilisation d'outils",
    "promptTemplate/title": "Mod√®le de prompt"
  },

  "llm.prediction.systemPrompt/title": "Invite syst√®me",
  "llm.prediction.systemPrompt/description": "Utilisez ce champ pour fournir des instructions g√©n√©rales au mod√®le, telles qu'un ensemble de r√®gles, de contraintes ou d'exigences.",
  "llm.prediction.systemPrompt/subTitle": "Directives pour l‚ÄôIA",
  "llm.prediction.temperature/title": "Temp√©rature",
  "llm.prediction.temperature/subTitle": "Niveau de hasard √† introduire. 0 produira le m√™me r√©sultat √† chaque fois, tandis que des valeurs plus √©lev√©es augmenteront la cr√©ativit√© et la variance.",
  "llm.prediction.temperature/info": "De la documentation llama.cpp : ¬´ La valeur par d√©faut est <{{dynamicValue}}>, qui offre un √©quilibre entre hasard et d√©terminisme. √Ä l'extr√™me, une temp√©rature de 0 choisira toujours le jeton suivant le plus probable, ce qui entra√Ænera des sorties identiques √† chaque ex√©cution ¬ª.",
  "llm.prediction.llama.sampling/title": "√âchantillonnage",
  "llm.prediction.topKSampling/title": "√âchantillonnage Top K",
  "llm.prediction.topKSampling/subTitle": "Limite le jeton suivant √† l'un des k jetons les plus probables. Agit de mani√®re similaire √† la temp√©rature.",
  "llm.prediction.topKSampling/info": "De la documentation llama.cpp :\n\nL‚Äô√©chantillonnage Top-k est une m√©thode de g√©n√©ration de texte qui s√©lectionne le jeton suivant uniquement parmi les k jetons les plus susceptibles pr√©dits par le mod√®le.\n\nIl aide √† r√©duire le risque de g√©n√©rer des jetons de faible probabilit√© ou absurdes, mais il peut √©galement limiter la diversit√© de la sortie.\n\nUne valeur plus √©lev√©e pour top-k (par exemple, 100) prendra en compte davantage de jetons et conduira √† un texte plus diversifi√©, tandis qu‚Äôune valeur inf√©rieure (par exemple, 10) se concentrera sur les jetons les plus probables et g√©n√©rera un texte plus conservateur.\n\n‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Threads CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Nombre de threads CPU √† utiliser pendant l'inf√©rence",
  "llm.prediction.llama.cpuThreads/info": "Le nombre de threads √† utiliser lors du calcul. L‚Äôaugmentation du nombre de threads ne se traduit pas toujours par une meilleure performance. La valeur par d√©faut est <{{dynamicValue}}>.",
  "llm.prediction.maxPredictedTokens/title": "Limiter la longueur de la r√©ponse",
  "llm.prediction.maxPredictedTokens/subTitle": "Optionnellement, plafonner la longueur de la r√©ponse de l‚ÄôIA",
  "llm.prediction.maxPredictedTokens/info": "Contr√¥lez la longueur maximale de la r√©ponse du chatbot. Activez pour d√©finir une limite sur la longueur maximale d'une r√©ponse ou d√©sactivez pour laisser le chatbot d√©cider quand arr√™ter.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Longueur maximale de la r√©ponse (jetons)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Environ {{maxWords}} mots",
  "llm.prediction.repeatPenalty/title": "P√©nalit√© de r√©p√©tition",
  "llm.prediction.repeatPenalty/subTitle": "Mesure dans laquelle d√©courager la r√©p√©tition du m√™me jeton",
  "llm.prediction.repeatPenalty/info": "De la documentation llama.cpp : ¬´ Aide √† emp√™cher le mod√®le de g√©n√©rer un texte r√©p√©titif ou monotone.\n\nUne valeur plus √©lev√©e (par exemple, 1,5) p√©nalisera davantage les r√©p√©titions, tandis qu‚Äôune valeur inf√©rieure (par exemple, 0,9) sera plus indulgente. ¬ª ‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "√âchantillonnage Min P",
  "llm.prediction.minPSampling/subTitle": "Probabilit√© de base minimale pour qu‚Äôun jeton soit s√©lectionn√© pour la sortie",
  "llm.prediction.minPSampling/info": "De la documentation llama.cpp :\n\nLa probabilit√© minimale √† consid√©rer pour un jeton, par rapport √† la probabilit√© du jeton le plus probable.\n\n‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "√âchantillonnage Top P",
  "llm.prediction.topPSampling/subTitle": "Probabilit√© cumulative minimale pour les jetons suivants possibles. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.topPSampling/info": "De la documentation llama.cpp :\n\nL‚Äô√©chantillonnage Top-p, √©galement connu sous le nom d‚Äô√©chantillonnage du noyau, est une autre m√©thode de g√©n√©ration de texte qui s√©lectionne le jeton suivant √† partir d‚Äôun ensemble de jetons dont la probabilit√© cumulative est au moins p.\n\nCette m√©thode offre un √©quilibre entre diversit√© et qualit√© en tenant compte √† la fois des probabilit√©s des jetons et du nombre de jetons √† √©chantillonner.\n\nUne valeur plus √©lev√©e pour top-p (par exemple, 0,95) conduira √† un texte plus diversifi√©, tandis qu‚Äôune valeur inf√©rieure (par exemple, 0,5) g√©n√©rera un texte plus cibl√© et conservateur. Doit √™tre compris entre (0, 1].\n\n‚Ä¢ La valeur par d√©faut est <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Cha√Ænes d‚Äôarr√™t",
  "llm.prediction.stopStrings/subTitle": "Cha√Ænes qui doivent arr√™ter le mod√®le de g√©n√©rer davantage de jetons",
  "llm.prediction.stopStrings/info": "Cha√Ænes sp√©cifiques qui, lorsqu'elles sont rencontr√©es, arr√™teront le mod√®le de g√©n√©rer d'autres jetons",
  "llm.prediction.stopStrings/placeholder": "Entrez une cha√Æne et appuyez sur ‚èé",
  "llm.prediction.contextOverflowPolicy/title": "Strat√©gie de d√©bordement du contexte",
  "llm.prediction.contextOverflowPolicy/subTitle": "Comment le mod√®le doit se comporter lorsque la conversation devient trop importante pour qu'il puisse la g√©rer",
  "llm.prediction.contextOverflowPolicy/info": "D√©cidez quoi faire lorsque la conversation d√©passe la taille de la m√©moire de travail du mod√®le (le ¬´ contexte ¬ª)",
  "llm.prediction.llama.frequencyPenalty/title": "P√©nalit√© de fr√©quence",
  "llm.prediction.llama.presencePenalty/title": "P√©nalit√© de pr√©sence",
  "llm.prediction.llama.tailFreeSampling/title": "√âchantillonnage sans queue",
  "llm.prediction.llama.locallyTypicalSampling/title": "√âchantillonnage localement typique",
  "llm.prediction.llama.xtcProbability/title": "Probabilit√© d‚Äô√©chantillonnage XTC",
  "llm.prediction.llama.xtcProbability/subTitle": "L'√©chantillonneur XTC (Exclure les meilleurs choix) ne sera activ√© qu'avec cette probabilit√© par jeton g√©n√©r√©. L'√©chantillonnage XTC peut stimuler la cr√©ativit√© et r√©duire les clich√©s",
  "llm.prediction.llama.xtcProbability/info": "L'√©chantillonneur XTC (Exclure les meilleurs choix) ne sera activ√© qu'avec cette probabilit√©, par jeton g√©n√©r√©. L'√©chantillonnage XTC stimule g√©n√©ralement la cr√©ativit√© et r√©duit les clich√©s",
  "llm.prediction.llama.xtcThreshold/title": "Seuil d‚Äô√©chantillonnage XTC",
  "llm.prediction.llama.xtcThreshold/subTitle": "Seuil XTC (Exclure les meilleurs choix). Avec une chance de `probabilit√©-xtc`, recherchez des jetons dont les probabilit√©s se situent entre le seuil `xtc` et 0,5, et supprimez tous ces jetons sauf le moins probable",
  "llm.prediction.llama.xtcThreshold/info": "Seuil XTC (Exclure les meilleurs choix). Avec une chance de `probabilit√©-xtc`, recherchez des jetons dont les probabilit√©s se situent entre le seuil `xtc` et 0,5, et supprimez tous ces jetons sauf le moins probable",
  "llm.prediction.mlx.topKSampling/title": "√âchantillonnage Top K",
  "llm.prediction.mlx.topKSampling/subTitle": "Limite le jeton suivant √† l'un des k jetons les plus probables. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.mlx.topKSampling/info": "Limite le jeton suivant √† l'un des k jetons les plus probables. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.onnx.topKSampling/title": "√âchantillonnage Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limite le jeton suivant √† l'un des k jetons les plus probables. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.onnx.topKSampling/info": "De la documentation ONNX :\n\nNombre du vocabulaire des jetons ayant les probabilit√©s les plus √©lev√©es √† conserver pour le filtrage top-k\n\n‚Ä¢ Ce filtre est d√©sactiv√© par d√©faut",
  "llm.prediction.onnx.repeatPenalty/title": "P√©nalit√© de r√©p√©tition",
  "llm.prediction.onnx.repeatPenalty/subTitle": "Mesure dans laquelle d√©courager la r√©p√©tition du m√™me jeton",
  "llm.prediction.onnx.repeatPenalty/info": "Une valeur plus √©lev√©e d√©courage le mod√®le de se r√©p√©ter.",
  "llm.prediction.onnx.topPSampling/title": "√âchantillonnage Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Probabilit√© cumulative minimale pour les jetons suivants possibles. Agit de mani√®re similaire √† la temp√©rature",
  "llm.prediction.onnx.topPSampling/info": "De la documentation ONNX :\n\nSeuls les jetons les plus probables dont les probabilit√©s s‚Äôadditionnent jusqu‚Äô√† TopP ou plus sont conserv√©s pour la g√©n√©ration.\n\n‚Ä¢ Ce filtre est d√©sactiv√© par d√©faut",
  "llm.prediction.seed/title": "Graine",
  "llm.prediction.structured/title": "Sortie structur√©e",
  "llm.prediction.structured/info": "Sortie structur√©e",
  "llm.prediction.structured/description": "Avanc√© : vous pouvez fournir un [sch√©ma JSON](https://json-schema.org/learn/miscellaneous-examples) pour imposer un format de sortie particulier au mod√®le. Consultez la [documentation](https://lmstudio.ai/docs/advanced/structured-output) pour en savoir plus",
  "llm.prediction.tools/title": "Utilisation d'outils",
  "llm.prediction.tools/description": "Avanc√© : vous pouvez fournir une liste JSON conforme des outils que le mod√®le peut demander √† appeler. Consultez la [documentation](https://lmstudio.ai/docs/advanced/tool-use) pour en savoir plus",
  "llm.prediction.tools/serverPageDescriptionAddon": "Passez ceci dans le corps de la requ√™te sous forme de `outils` lors de l'utilisation de l'API serveur",
  "llm.prediction.promptTemplate/title": "Mod√®le de prompt",
  "llm.prediction.promptTemplate/subTitle": "Le format dans lequel les messages dans la conversation sont envoy√©s au mod√®le. La modification de ce param√®tre peut entra√Æner un comportement inattendu - assurez-vous de savoir ce que vous faites !",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "Nombre de jetons brouillon",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "Le nombre de jetons √† g√©n√©rer avec le mod√®le brouillon par jeton du mod√®le principal. Trouvez le juste √©quilibre entre puissance de calcul et r√©compense.",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "Seuil d'arr√™t du brouillon",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "Continuer le brouillon jusqu‚Äô√† ce que la probabilit√© d‚Äôun jeton passe en dessous de ce seuil. Des valeurs plus √©lev√©es signifient g√©n√©ralement un risque moindre et une r√©compense moindre.",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "Taille minimale du brouillon",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "Les brouillons de taille inf√©rieure √† celle-ci seront ignor√©s par le mod√®le principal. Des valeurs plus √©lev√©es signifient g√©n√©ralement un risque moindre et une r√©compense moindre.",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "Taille maximale du brouillon",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "Nombre maximal de jetons autoris√©s dans un brouillon. Plafond si toutes les probabilit√©s des jetons sont > le seuil. Des valeurs plus faibles signifient g√©n√©ralement un risque moindre et une r√©compense moindre.",
  "llm.prediction.speculativeDecoding.draftModel/title": "Mod√®le brouillon",
  "llm.prediction.reasoning.parsing/title": "Analyse de la section raisonnement",
  "llm.prediction.reasoning.parsing/subTitle": "Comment analyser les sections de raisonnement dans la sortie du mod√®le.",

  "llm.load.contextLength/title": "Longueur du contexte",
  "llm.load.contextLength/subTitle": "Le nombre maximal de jetons que le mod√®le peut prendre en compte dans une seule invite. Consultez les options de d√©bordement de conversation sous ¬´ Param√®tres d‚Äôinf√©rence ¬ª pour plus de fa√ßons de g√©rer cela.",
  "llm.load.contextLength/info": "Sp√©cifie le nombre maximal de jetons que le mod√®le peut consid√©rer √† la fois, ce qui affecte la quantit√© de contexte qu'il retient pendant le traitement",
  "llm.load.contextLength/warning": "D√©finir une valeur √©lev√©e pour la longueur du contexte peut avoir un impact significatif sur l‚Äôutilisation de la m√©moire.",
  "llm.load.seed/title": "Graine",
  "llm.load.seed/subTitle": "La graine du g√©n√©rateur de nombres al√©atoires utilis√©e pour la g√©n√©ration de texte. -1 est une graine al√©atoire",
  "llm.load.seed/info": "Graine al√©atoire : d√©finit la graine pour la g√©n√©ration de nombres al√©atoires afin d‚Äôassurer des r√©sultats reproductibles.",

  "llm.load.llama.evalBatchSize/title": "Taille du lot d'√©valuation",
  "llm.load.llama.evalBatchSize/subTitle": "Nombre de jetons d'entr√©e √† traiter √† la fois. L'augmentation de ce param√®tre augmente les performances au d√©triment de l‚Äôutilisation de la m√©moire.",
  "llm.load.llama.evalBatchSize/info": "D√©finit le nombre d‚Äôexemples trait√©s ensemble dans un seul lot pendant l‚Äô√©valuation",
  "llm.load.llama.ropeFrequencyBase/title": "Fr√©quence RoPE de base",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Fr√©quence de base personnalis√©e pour les int√©grations rotatives positionnelles (RoPE). L‚Äôaugmentation de ce param√®tre peut permettre de meilleures performances √† des longueurs de contexte √©lev√©es.",
  "llm.load.llama.ropeFrequencyBase/info": "[Avanc√©] Ajuste la fr√©quence de base pour l‚Äôencodage positionnel rotatif, affectant la fa√ßon dont les informations de position sont int√©gr√©es",
  "llm.load.llama.ropeFrequencyScale/title": "√âchelle de fr√©quence RoPE",
  "llm.load.llama.ropeFrequencyScale/subTitle": "La longueur du contexte est mise √† l‚Äô√©chelle par ce facteur pour √©tendre le contexte efficace √† l‚Äôaide de RoPE.",
  "llm.load.llama.ropeFrequencyScale/info": "[Avanc√©] Modifie la mise √† l‚Äô√©chelle de la fr√©quence pour l‚Äôencodage positionnel rotatif afin de contr√¥ler la granularit√© de l‚Äôencodage positionnel",
  "llm.load.llama.acceleration.offloadRatio/title": "D√©chargement GPU",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Nombre de couches de mod√®le discr√®tes √† calculer sur le GPU pour l‚Äôacc√©l√©ration GPU.",
  "llm.load.llama.acceleration.offloadRatio/info": "D√©finissez le nombre de couches √† d√©charger vers le GPU.",
  "llm.load.llama.flashAttention/title": "Flash Attention",
  "llm.load.llama.flashAttention/subTitle": "R√©duit l‚Äôutilisation de la m√©moire et le temps de g√©n√©ration sur certains mod√®les.",
  "llm.load.llama.flashAttention/info": "Acc√©l√®re les m√©canismes d‚Äôattention pour un traitement plus rapide et plus efficace",
  "llm.load.numExperts/title": "Nombre d'experts",
  "llm.load.numExperts/subTitle": "Nombre d'experts √† utiliser dans le mod√®le.",
  "llm.load.numExperts/info": "Le nombre d‚Äôexperts √† utiliser dans le mod√®le.",
  "llm.load.llama.keepModelInMemory/title": "Garder le mod√®le en m√©moire",
  "llm.load.llama.keepModelInMemory/subTitle": "R√©serve de la m√©moire syst√®me pour le mod√®le, m√™me lorsqu‚Äôil est d√©charg√© sur le GPU. Am√©liore les performances mais n√©cessite plus de RAM syst√®me.",
  "llm.load.llama.keepModelInMemory/info": "Emp√™che le mod√®le d‚Äô√™tre √©chang√© sur disque, garantissant un acc√®s plus rapide au prix d‚Äôune utilisation accrue de la RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Utiliser FP16 pour le cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "R√©duit l‚Äôutilisation de la m√©moire en stockant le cache en pr√©cision demi-mot (FP16)",
  "llm.load.llama.tryMmap/title": "Essayer mmap()",
  "llm.load.llama.tryMmap/subTitle": "Am√©liore le temps de chargement du mod√®le. La d√©sactivation de ce param√®tre peut am√©liorer les performances lorsque le mod√®le est plus grand que la RAM syst√®me disponible.",
  "llm.load.llama.tryMmap/info": "Charge les fichiers mod√®les directement √† partir du disque en m√©moire",
  "llm.load.llama.cpuThreadPoolSize/title": "Taille du pool de threads CPU",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "Nombre de threads CPU √† allouer au pool de threads utilis√© pour le calcul du mod√®le.",
  "llm.load.llama.cpuThreadPoolSize/info": "Le nombre de threads CPU √† allouer au pool de threads utilis√© pour le calcul du mod√®le. L‚Äôaugmentation du nombre de threads ne se traduit pas toujours par une meilleure performance. La valeur par d√©faut est <{{dynamicValue}}>.",
  "llm.load.llama.kCacheQuantizationType/title": "Type de quantification du cache K",
  "llm.load.llama.kCacheQuantizationType/subTitle": "Les valeurs inf√©rieures r√©duisent l‚Äôutilisation de la m√©moire mais peuvent diminuer la qualit√©. L‚Äôeffet varie consid√©rablement d‚Äôun mod√®le √† l‚Äôautre.",
  "llm.load.llama.vCacheQuantizationType/title": "Type de quantification du cache V",
  "llm.load.llama.vCacheQuantizationType/subTitle": "Les valeurs inf√©rieures r√©duisent l‚Äôutilisation de la m√©moire mais peuvent diminuer la qualit√©. L‚Äôeffet varie consid√©rablement d‚Äôun mod√®le √† l‚Äôautre.",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "‚ö†Ô∏è Vous devez d√©sactiver cette valeur si Flash Attention n‚Äôest pas activ√©",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "Ne peut √™tre activ√©e que lorsque Flash Attention est activ√©",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "‚ö†Ô∏è Vous devez d√©sactiver flash attention lors de l‚Äôutilisation de F32",
  "llm.load.mlx.kvCacheBits/title": "Quantification du cache KV",
  "llm.load.mlx.kvCacheBits/subTitle": "Nombre de bits √† quantifier le cache KV.",
  "llm.load.mlx.kvCacheBits/info": "Nombre de bits √† quantifier le cache KV.",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "Le param√®tre Longueur du contexte est ignor√© lors de l‚Äôutilisation de la quantification du cache KV",
  "llm.load.mlx.kvCacheGroupSize/title": "Quantification du cache KV : Taille du groupe",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "Taille de groupe pendant l‚Äôop√©ration de quantification pour le cache KV. Une taille de groupe plus √©lev√©e r√©duit l‚Äôutilisation de la m√©moire mais peut diminuer la qualit√©.",
  "llm.load.mlx.kvCacheGroupSize/info": "Nombre de bits √† quantifier le cache KV.",
  "llm.load.mlx.kvCacheQuantizationStart/title": "Quantification du cache KV : D√©marrer lorsque ctx atteint cette longueur",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "Seuil de longueur de contexte pour commencer la quantification du cache KV.",
  "llm.load.mlx.kvCacheQuantizationStart/info": "Seuil de longueur de contexte pour commencer la quantification du cache KV.",
  "llm.load.mlx.kvCacheQuantization/title": "Quantification du cache KV",
  "llm.load.mlx.kvCacheQuantization/subTitle": "Quantifier le cache KV du mod√®le. Cela peut entra√Æner une g√©n√©ration plus rapide et une empreinte m√©moire r√©duite, au d√©triment de la qualit√© de la sortie du mod√®le.",
  "llm.load.mlx.kvCacheQuantization/bits/title": "Bits de quantification du cache KV",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "Nombre de bits √† quantifier le cache KV.",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "Bits",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "Strat√©gie de taille de groupe",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "Pr√©cision",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "√âquilibr√©",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "Rapide",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "Avanc√© : Configuration de la strat√©gie de taille de groupe quantifi√©e\n\n‚Ä¢ Pr√©cision = taille du groupe 32\n‚Ä¢ √âquilibr√© = taille du groupe 64\n‚Ä¢ Rapide = taille du groupe 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "D√©marrer la quantification lorsque le ctx atteint cette longueur",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "Lorsque le contexte atteint ce nombre de jetons,\ncommencez √† quantifier le cache KV",

  "embedding.load.contextLength/title": "Longueur du contexte",
  "embedding.load.contextLength/subTitle": "Le nombre maximal de jetons que le mod√®le peut prendre en compte dans une seule invite. Consultez les options de d√©bordement de conversation sous ¬´ Param√®tres d‚Äôinf√©rence ¬ª pour plus de fa√ßons de g√©rer cela.",
  "embedding.load.contextLength/info": "Sp√©cifie le nombre maximal de jetons que le mod√®le peut consid√©rer √† la fois, ce qui affecte la quantit√© de contexte qu‚Äôil retient pendant le traitement",
  "embedding.load.llama.ropeFrequencyBase/title": "Fr√©quence RoPE de base",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Fr√©quence de base personnalis√©e pour les int√©grations rotatives positionnelles (RoPE). L‚Äôaugmentation de ce param√®tre peut permettre de meilleures performances √† des longueurs de contexte √©lev√©es.",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avanc√©] Ajuste la fr√©quence de base pour l‚Äôencodage positionnel rotatif, affectant la fa√ßon dont les informations de position sont int√©gr√©es",
  "embedding.load.llama.evalBatchSize/title": "Taille du lot d'√©valuation",
  "embedding.load.llama.evalBatchSize/subTitle": "Nombre de jetons d'entr√©e √† traiter √† la fois. L'augmentation de ce param√®tre augmente les performances au d√©triment de l‚Äôutilisation de la m√©moire.",
  "embedding.load.llama.evalBatchSize/info": "D√©finit le nombre de jetons trait√©s ensemble dans un seul lot pendant l‚Äô√©valuation",
  "embedding.load.llama.ropeFrequencyScale/title": "√âchelle de fr√©quence RoPE",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "La longueur du contexte est mise √† l‚Äô√©chelle par ce facteur pour √©tendre le contexte efficace √† l‚Äôaide de RoPE.",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avanc√©] Modifie la mise √† l‚Äô√©chelle de la fr√©quence pour l‚Äôencodage positionnel rotatif afin de contr√¥ler la granularit√© de l‚Äôencodage positionnel",
  "embedding.load.llama.acceleration.offloadRatio/title": "D√©chargement GPU",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Nombre de couches de mod√®le discr√®tes √† calculer sur le GPU pour l‚Äôacc√©l√©ration GPU.",
  "embedding.load.llama.acceleration.offloadRatio/info": "D√©finissez le nombre de couches √† d√©charger vers le GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Garder le mod√®le en m√©moire",
  "embedding.load.llama.keepModelInMemory/subTitle": "R√©serve de la m√©moire syst√®me pour le mod√®le, m√™me lorsqu‚Äôil est d√©charg√© sur le GPU. Am√©liore les performances mais n√©cessite plus de RAM syst√®me.",
  "embedding.load.llama.keepModelInMemory/info": "Emp√™che le mod√®le d‚Äô√™tre √©chang√© sur disque, garantissant un acc√®s plus rapide au prix d‚Äôune utilisation accrue de la RAM",
  "embedding.load.llama.tryMmap/title": "Essayer mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Am√©liore le temps de chargement du mod√®le. La d√©sactivation de ce param√®tre peut am√©liorer les performances lorsque le mod√®le est plus grand que la RAM syst√®me disponible.",
  "embedding.load.llama.tryMmap/info": "Charge les fichiers mod√®les directement √† partir du disque en m√©moire",
  "embedding.load.seed/title": "Graine",
  "embedding.load.seed/subTitle": "La graine du g√©n√©rateur de nombres al√©atoires utilis√©e pour la g√©n√©ration de texte. -1 est une graine al√©atoire",

  "embedding.load.seed/info": "Graine al√©atoire : d√©finit la graine pour la g√©n√©ration de nombres al√©atoires afin d‚Äôassurer des r√©sultats reproductibles.",

  "presetTooltip": {
    "included/title": "Valeurs par d√©faut",
    "included/description": "Les champs suivants seront appliqu√©s.",
    "included/empty": "Aucun champ de ce pr√©r√©glage ne s'applique dans ce contexte.",
    "included/conflict": "Vous serez invit√©(e) √† choisir si vous souhaitez appliquer cette valeur.",
    "separateLoad/title": "Configuration au chargement",
    "separateLoad/description.1": "Ce pr√©r√©glage inclut √©galement la configuration de chargement suivante. Les configurations de chargement sont globales au mod√®le et n√©cessitent un rechargement du mod√®le pour prendre effet. Maintenez",
    "separateLoad/description.2": "pour appliquer √†",
    "separateLoad/description.3": ".",
    "excluded/title": "Peut ne pas s'appliquer",
    "excluded/description": "Les champs suivants sont inclus dans le pr√©r√©glage mais ne s'appliquent pas dans le contexte actuel.",
    "legacy/title": "Pr√©r√©glage obsol√®te",
    "legacy/description": "Ce pr√©r√©glage est un pr√©r√©glage obsol√®te. Il inclut les champs suivants qui sont soit g√©r√©s automatiquement maintenant, soit ne sont plus applicables.",
    "button/publish": "Publier sur le Hub",
    "button/pushUpdate": "Pousser les modifications vers le Hub",
    "button/export": "Exporter"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<Vide>"
    },
    "checkboxNumeric": {
      "off": "D√âSACTIV√â"
    },
    "llamaCacheQuantizationType": {
      "off": "D√âSACTIV√â"
    },
    "mlxKvCacheBits": {
      "off": "D√âSACTIV√â"
    },
    "stringArray": {
      "empty": "<Vide>"
    },
    "llmPromptTemplate": {
      "type": "Type",
      "types.jinja/label": "Mod√®le (Jinja)",
      "jinja.bosToken/label": "Jeton de d√©but (BOS)",
      "jinja.eosToken/label": "Jeton de fin (EOS)",
      "jinja.template/label": "Mod√®le",
      "jinja/error": "Erreur lors de l'analyse du mod√®le Jinja : {{error}}",
      "jinja/empty": "Veuillez entrer un mod√®le Jinja ci-dessus.",
      "jinja/unlikelyToWork": "Le mod√®le Jinja que vous avez fourni ci-dessus est peu susceptible de fonctionner car il ne fait pas r√©f√©rence √† la variable \"messages\". V√©rifiez si vous avez entr√© un mod√®le correct.",
      "types.manual/label": "Manuel",
      "manual.subfield.beforeSystem/label": "Avant le syst√®me",
      "manual.subfield.beforeSystem/placeholder": "Entrez le pr√©fixe du syst√®me...",
      "manual.subfield.afterSystem/label": "Apr√®s le syst√®me",
      "manual.subfield.afterSystem/placeholder": "Entrez le suffixe du syst√®me...",
      "manual.subfield.beforeUser/label": "Avant l'utilisateur",
      "manual.subfield.beforeUser/placeholder": "Entrez le pr√©fixe de l'utilisateur...",
      "manual.subfield.afterUser/label": "Apr√®s l'utilisateur",
      "manual.subfield.afterUser/placeholder": "Entrez le suffixe de l'utilisateur...",
      "manual.subfield.beforeAssistant/label": "Avant l'assistant",
      "manual.subfield.beforeAssistant/placeholder": "Entrez le pr√©fixe de l'assistant...",
      "manual.subfield.afterAssistant/label": "Apr√®s l'assistant",
      "manual.subfield.afterAssistant/placeholder": "Entrez le suffixe de l'assistant...",
      "stopStrings/label": "Cha√Ænes d'arr√™t suppl√©mentaires",
      "stopStrings/subTitle": "Cha√Ænes d'arr√™t sp√©cifiques au mod√®le qui seront utilis√©es en plus des cha√Ænes d'arr√™t sp√©cifi√©es par l'utilisateur."
    },
    "contextLength": {
      "maxValueTooltip": "Il s'agit du nombre maximal de jetons que le mod√®le a √©t√© entra√Æn√© √† g√©rer. Cliquez pour d√©finir le contexte sur cette valeur.",
      "maxValueTextStart": "Le mod√®le prend en charge jusqu'√†",
      "maxValueTextEnd": "jetons",
      "tooltipHint": "Bien qu'un mod√®le puisse prendre en charge un certain nombre de jetons, les performances peuvent se d√©t√©riorer si les ressources de votre machine ne suffisent pas √† g√©rer la charge - soyez prudent lorsque vous augmentez cette valeur."
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "Arr√™ter √† la limite",
      "stopAtLimitSub": "Arr√™tez la g√©n√©ration une fois que la m√©moire du mod√®le est pleine.",
      "truncateMiddle": "Tronquer le milieu",
      "truncateMiddleSub": "Supprime les messages du milieu de la conversation pour faire de la place aux nouveaux. Le mod√®le se souviendra toujours du d√©but de la conversation.",
      "rollingWindow": "Fen√™tre glissante",
      "rollingWindowSub": "Le mod√®le n'aura que les quelques derniers messages, mais il peut oublier le d√©but de la conversation."
    },
    "llamaAccelerationOffloadRatio": {
      "max": "MAX",
      "off": "D√âSACTIV√â"
    },
    "llamaAccelerationSplitStrategy": {
      "evenly": "Uniform√©ment",
      "favorMainGpu": "Privil√©gier le GPU principal"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "En savoir plus sur son fonctionnement",
      "placeholder": "S√©lectionnez un mod√®le de brouillon compatible",
      "noCompatible": "Aucun mod√®le de brouillon compatible trouv√© pour votre s√©lection de mod√®le actuelle.",
      "stillLoading": "Identification des mod√®les de brouillon compatibles...",
      "notCompatible": "Le mod√®le de brouillon s√©lectionn√© (<draft/>) n'est pas compatible avec la s√©lection de mod√®le actuelle (<current/>).",
      "off": "D√âSACTIV√â",
      "loadModelToSeeOptions": "Chargez le mod√®le <raccourci clavier /> pour voir les options compatibles.",
      "compatibleWithNumberOfModels": "Recommand√© pour au moins {{dynamicValue}} de vos mod√®les",
      "recommendedForSomeModels": "Recommand√© pour certains mod√®les",
      "recommendedForLlamaModels": "Recommand√© pour les mod√®les Llama",
      "recommendedForQwenModels": "Recommand√© pour les mod√®les Qwen",
      "onboardingModal": {
        "introducing": "Pr√©sentation de",
        "speculativeDecoding": "D√©codage sp√©culatif",
        "firstStepBody": "Acc√©l√©ration de l'inf√©rence pour les mod√®les <custom-span>llama.cpp</custom-span> et <custom-span>MLX</custom-span>",
        "secondStepTitle": "Acc√©l√©ration de l'inf√©rence avec le d√©codage sp√©culatif",
        "secondStepBody": "Le d√©codage sp√©culatif est une technique impliquant la collaboration de deux mod√®les :\n - Un mod√®le \"principal\" plus grand\n - Un mod√®le \"brouillon\" plus petit\n\nPendant la g√©n√©ration, le mod√®le brouillon propose rapidement des jetons que le mod√®le principal doit v√©rifier. La v√©rification des jetons est un processus beaucoup plus rapide que leur g√©n√©ration r√©elle, ce qui explique les gains de vitesse. **Plus la diff√©rence de taille entre le mod√®le principal et le mod√®le brouillon est grande, plus l'acc√©l√©ration est importante.**.\n\nAfin de maintenir la qualit√©, le mod√®le principal n'accepte que les jetons qui correspondent √† ce qu'il aurait g√©n√©r√© lui-m√™me, permettant ainsi la r√©ponse du grand mod√®le √† des vitesses d'inf√©rence plus rapides. Les deux mod√®les doivent partager le m√™me vocabulaire.",
        "draftModelRecommendationsTitle": "Recommandations de mod√®les brouillon",
        "basedOnCurrentModels": "Bas√© sur vos mod√®les actuels",
        "close": "Fermer",
        "next": "Suivant",
        "done": "Termin√©"
      },
      "speculativeDecodingLoadModelToSeeOptions": "Veuillez d'abord charger un mod√®le <keyboard-shortcut /> pour voir les options compatibles.",
      "errorEngineNotSupported": "Le d√©codage sp√©culatif n√©cessite au moins la version {{minVersion}} du moteur {{engineName}}. Veuillez mettre √† jour le moteur (<key/>) et recharger le mod√®le pour utiliser cette fonctionnalit√©.",
      "errorEngineNotSupported/noKey": "Le d√©codage sp√©culatif n√©cessite au moins la version {{minVersion}} du moteur {{engineName}}. Veuillez mettre √† jour le moteur et recharger le mod√®le pour utiliser cette fonctionnalit√©."
    },
    "llmReasoningParsing": {
      "startString/label": "Cha√Æne de d√©but",
      "startString/placeholder": "Entrez la cha√Æne de d√©but...",
      "endString/label": "Cha√Æne de fin",
      "endString/placeholder": "Entrez la cha√Æne de fin..."
    }
  },
  "saveConflictResolution": {
    "title": "Choisissez les valeurs √† inclure dans le pr√©r√©glage",
    "description": "S√©lectionnez et choisissez les valeurs √† conserver.",
    "instructions": "Cliquez sur une valeur pour l'inclure.",
    "userValues": "Valeur pr√©c√©dente",
    "presetValues": "Nouvelle valeur",
    "confirm": "Confirmer",
    "cancel": "Annuler"
  },
  "applyConflictResolution": {
    "title": "Quelles valeurs conserver ?",
    "description": "Vous avez des modifications non valid√©es qui se chevauchent avec le pr√©r√©glage entrant.",
    "instructions": "Cliquez sur une valeur pour la conserver.",
    "userValues": "Valeur actuelle",
    "presetValues": "Nouvelle valeur du pr√©r√©glage",
    "confirm": "Confirmer",
    "cancel": "Annuler"
  },
  "empty": "<Vide>",
  "noModelSelected": "Aucun mod√®le s√©lectionn√©",
  "apiIdentifier.label": "Identifiant de l'API",
  "apiIdentifier.hint": "Fournissez √©ventuellement un identifiant pour ce mod√®le. Il sera utilis√© dans les requ√™tes API. Laissez vide pour utiliser l'identifiant par d√©faut.",
  "idleTTL.label": "D√©chargement automatique en cas d'inactivit√© (TTL)",
  "idleTTL.hint": "Si d√©fini, le mod√®le sera automatiquement d√©charg√© apr√®s √™tre rest√© inactif pendant la dur√©e sp√©cifi√©e.",
  "idleTTL.mins": "min",

  "presets": {
    "title": "Pr√©r√©glage",
    "commitChanges": "Valider les modifications",
    "commitChanges/description": "Validez vos modifications dans le pr√©r√©glage.",
    "commitChanges.manual": "Nouveaux champs d√©tect√©s. Vous pourrez choisir quelles modifications inclure dans le pr√©r√©glage.",
    "commitChanges.manual.hold.0": "Maintenez",
    "commitChanges.manual.hold.1": "pour choisir les modifications √† valider dans le pr√©r√©glage.",
    "commitChanges.saveAll.hold.0": "Maintenez",
    "commitChanges.saveAll.hold.1": "pour enregistrer toutes les modifications.",
    "commitChanges.saveInPreset.hold.0": "Maintenez",
    "commitChanges.saveInPreset.hold.1": "pour n'enregistrer que les modifications apport√©es aux champs d√©j√† inclus dans le pr√©r√©glage.",
    "commitChanges/error": "La validation des modifications du pr√©r√©glage a √©chou√©.",
    "commitChanges.manual/description": "Choisissez les modifications √† inclure dans le pr√©r√©glage.",
    "saveAs": "Enregistrer sous...",
    "presetNamePlaceholder": "Entrez un nom pour le pr√©r√©glage...",
    "cannotCommitChangesLegacy": "Ce pr√©r√©glage est obsol√®te et ne peut pas √™tre modifi√©. Vous pouvez cr√©er une copie en utilisant \"Enregistrer sous...\".",
    "cannotCommitChangesNoChanges": "Aucune modification √† valider.",
    "emptyNoUnsaved": "S√©lectionner un pr√©r√©glage...",
    "emptyWithUnsaved": "Pr√©r√©glage non enregistr√©",
    "saveEmptyWithUnsaved": "Enregistrer le pr√©r√©glage sous...",
    "save": "Enregistrer",
    "cancel": "Annuler",
    "saving": "Enregistrement en cours...",
    "save/error": "L'enregistrement du pr√©r√©glage a √©chou√©.",
    "deselect": "D√©s√©lectionner le pr√©r√©glage",
    "deselect/error": "La d√©s√©lection du pr√©r√©glage a √©chou√©.",
    "select/error": "La s√©lection du pr√©r√©glage a √©chou√©.",
    "delete/error": "La suppression du pr√©r√©glage a √©chou√©.",
    "discardChanges": "Annuler les modifications non enregistr√©es",
    "discardChanges/info": "Annulez toutes les modifications non valid√©es et restaurez le pr√©r√©glage √† son √©tat d'origine.",
    "newEmptyPreset": "+ Nouveau pr√©r√©glage",
    "importPreset": "Importer",
    "contextMenuSelect": "Appliquer le pr√©r√©glage",
    "contextMenuDelete": "Supprimer...",
    "contextMenuShare": "Publier...",
    "contextMenuOpenInHub": "Voir sur le Hub",
    "contextMenuPushChanges": "Pousser les modifications vers le Hub",
    "contextMenuPushingChanges": "En cours de pouss√©e...",
    "contextMenuPushedChanges": "Modifications pouss√©es",
    "contextMenuExport": "Exporter le fichier",
    "contextMenuRevealInExplorer": "Afficher dans l'explorateur de fichiers",
    "contextMenuRevealInFinder": "Afficher dans Finder",
    "share": {
      "title": "Publier un pr√©r√©glage",
      "action": "Partagez votre pr√©r√©glage pour que d'autres puissent le t√©l√©charger, l'aimer et le forker.",
      "presetOwnerLabel": "Propri√©taire",
      "uploadAs": "Votre pr√©r√©glage sera cr√©√© sous le nom de {{name}}",
      "presetNameLabel": "Nom du pr√©r√©glage",
      "descriptionLabel": "Description (facultatif)",
      "loading": "Publication en cours...",
      "success": "Pr√©r√©glage publi√© avec succ√®s",
      "presetIsLive": "<preset-name /> est maintenant disponible sur le Hub !",
      "close": "Fermer",
      "confirmViewOnWeb": "Voir sur le web",
      "confirmCopy": "Copier l'URL",
      "confirmCopied": "Copi√© !",
      "pushedToHub": "Votre pr√©r√©glage a √©t√© pouss√© vers le Hub",
      "descriptionPlaceholder": "Entrez une description...",
      "willBePublic": "La publication de votre pr√©r√©glage le rendra public.",
      "publicSubtitle": "Votre pr√©r√©glage est <custom-bold>public</custom-bold>. D'autres peuvent le t√©l√©charger et le forker sur lmstudio.ai",
      "confirmShareButton": "Publier",
      "error": "La publication du pr√©r√©glage a √©chou√©.",
      "createFreeAccount": "Cr√©ez un compte gratuit sur le Hub pour publier des pr√©r√©glages."
    },
    "update": {
      "title": "Pousser les modifications vers le Hub",
      "title/success": "Pr√©r√©glage mis √† jour avec succ√®s",
      "subtitle": "Apportez des modifications √† <custom-preset-name /> et poussez-les vers le Hub.",
      "descriptionLabel": "Description",
      "descriptionPlaceholder": "Entrez une description...",
      "loading": "Pouss√©e en cours...",
      "cancel": "Annuler",
      "createFreeAccount": "Cr√©ez un compte gratuit sur le Hub pour publier des pr√©r√©glages.",
      "error": "La mise √† jour a √©chou√©.",
      "confirmUpdateButton": "Pousser"
    },
    "import": {
      "title": "Importer un pr√©r√©glage depuis un fichier",
      "dragPrompt": "Faites glisser et d√©posez les fichiers JSON de pr√©r√©glage ou <custom-link>s√©lectionnez √† partir de votre ordinateur</custom-link>",
      "remove": "Supprimer",
      "cancel": "Annuler",
      "importPreset_zero": "Importer un pr√©r√©glage",
      "importPreset_one": "Importer un pr√©r√©glage",
      "importPreset_other": "Importer {{count}} pr√©r√©glages",
      "selectDialog": {
        "title": "S√©lectionner le fichier de pr√©r√©glage (.json)",
        "button": "Importer"
      },
      "error": "L'importation du pr√©r√©glage a √©chou√©.",
      "resultsModal": {
        "titleSuccessSection_one": "1 pr√©r√©glage import√© avec succ√®s",
        "titleSuccessSection_other": "{{count}} pr√©r√©glages import√©s avec succ√®s",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} √©chec)",
        "titleFailSection_other": "({{count}} √©checs)",
        "titleAllFailed": "√âchec de l'importation des pr√©r√©glages",
        "importMore": "Importer plus",
        "close": "Termin√©",
        "successBadge": "Succ√®s",
        "alreadyExistsBadge": "Pr√©r√©glage existe d√©j√†",
        "errorBadge": "Erreur",
        "invalidFileBadge": "Fichier invalide",
        "otherErrorBadge": "√âchec de l'importation du pr√©r√©glage",
        "errorViewDetailsButton": "Voir les d√©tails",
        "seeError": "Voir l'erreur",
        "noName": "Aucun nom de pr√©r√©glage",
        "useInChat": "Utiliser dans la conversation"
      },
      "importFromUrl": {
        "button": "Importer depuis une URL...",
        "title": "Importer depuis une URL",
        "back": "Importer depuis un fichier...",
        "action": "Collez l'URL du Hub LM Studio du pr√©r√©glage que vous souhaitez importer ci-dessous.",
        "invalidUrl": "URL invalide. Assurez-vous de coller une URL correcte du Hub LM Studio.",
        "tip": "Vous pouvez installer le pr√©r√©glage directement avec le bouton {{buttonName}} dans le Hub LM Studio",
        "confirm": "Importer",
        "cancel": "Annuler",
        "loading": "Importation en cours...",
        "error": "Le t√©l√©chargement du pr√©r√©glage a √©chou√©."
      }
    },
    "download": {
      "title": "T√©l√©charger <preset-name /> depuis le Hub LM Studio",
      "subtitle": "Enregistrez <custom-name /> dans vos pr√©r√©glages. Cela vous permettra d'utiliser ce pr√©r√©glage dans l'application.",
      "button": "T√©l√©charger",
      "button/loading": "T√©l√©chargement en cours...",
      "cancel": "Annuler",
      "error": "Le t√©l√©chargement du pr√©r√©glage a √©chou√©."
    },
    "inclusiveness": {
      "speculativeDecoding": "Inclure dans le pr√©r√©glage"
    }
  },

  "flashAttentionWarning": "Flash Attention est une fonctionnalit√© exp√©rimentale qui peut entra√Æner des probl√®mes avec certains mod√®les. Si vous rencontrez des probl√®mes, essayez de la d√©sactiver.",
  "llamaKvCacheQuantizationWarning": "La quantification KV Cache est une fonctionnalit√© exp√©rimentale qui peut entra√Æner des probl√®mes avec certains mod√®les. Flash Attention doit √™tre activ√© pour la quantification V. Si vous rencontrez des probl√®mes, r√©initialisez aux valeurs par d√©faut \"F16\".",

  "seedUncheckedHint": "Graine al√©atoire",
  "ropeFrequencyBaseUncheckedHint": "Auto",
  "ropeFrequencyScaleUncheckedHint": "Auto",

  "hardware": {
    "advancedGpuSettings": "Param√®tres GPU avanc√©s",
    "advancedGpuSettings.info": "Si vous n'√™tes pas s√ªr, laissez ces valeurs par d√©faut.",
    "advancedGpuSettings.reset": "R√©initialiser aux valeurs par d√©faut",
    "environmentVariables": {
      "title": "Variables d'environnement",
      "description": "Variables d'environnement actives pendant la dur√©e de vie du mod√®le.",
      "key.placeholder": "S√©lectionner une variable...",
      "value.placeholder": "Valeur"
    },
    "mainGpu": {
      "title": "GPU principal",
      "description": "Le GPU √† privil√©gier pour le calcul du mod√®le.",
      "placeholder": "S√©lectionner le GPU principal..."
    },
    "splitStrategy": {
      "title": "Strat√©gie de r√©partition",
      "description": "Comment r√©partir le calcul du mod√®le entre les GPU.",
      "placeholder": "S√©lectionner une strat√©gie de r√©partition..."
    }
  }
}
